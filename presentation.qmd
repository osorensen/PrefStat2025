---
title: "Bayesian Inference with the Mallows Model"
author: "Øystein Sørensen"
format: 
  revealjs:
    theme: serif
    echo: true
engine: knitr
filters:
  - webr
editor: source
bibliography: references.bib
---

## Learning goals

- Bayesian Mallows model [@vitelliProbabilisticPreferenceLearning2017]
- `BayesMallows` [@sorensenBayesMallowsPackageBayesian2020]
- Computational algorithms [@vitelliProbabilisticPreferenceLearning2017]
- Non-transitive preferences [@crispinoBayesianMallowsApproach2019]


## R setup

```{r, message=FALSE}
library(tidyverse)
theme_set(theme_bw())
theme_update(panel.grid = element_blank())
library(BayesMallows)
library(parallel)
library(e1071)
```

# Bayesian Mallows model

## Data-generating distribution

Ranking $r$ generated by

$$
p\left(r | \alpha, \rho\right) = \frac{\exp\left\{ - \left(\alpha /m\right) d\left(r, \rho\right)\right\}}{Z\left(\alpha \right)} 
$$

with support $\mathcal{P}_{m}$.

## Data-generating distribution

- Try out different $\alpha$ and $\rho$:

```{webr-r}
set.seed(12)
sample_mallows(rho0 = 1:5, alpha0 = 2, n_samples = 4)
```


## Bayesian inference in a nutshell

- Parameters of interest $\theta$.

- Prior distribution $\pi(\theta)$.

- Likelihood $p(\text{data} | \theta)$.

. . .

- Posterior 

$$
p(\theta | \text{data}) = \frac{\pi(\theta) p(\text{data} | \theta)}{p(\text{data})}
$$ 

## Priors for scale parameter

-   @crispinoBayesianMallowsApproach2019:

$$
\pi\left(\alpha\right) \propto \alpha^{\gamma-1} \exp\{-\lambda \alpha\}, ~ \alpha \in [0, \infty)
$$

-   @vitelliProbabilisticPreferenceLearning2017, special case $\gamma=1$.

## Gamma prior

$\gamma$ is the *shape* and $\lambda$ is the *rate*.

```{r, echo=FALSE}
crossing(
  alpha = seq(from = 0, to = 3, by = .1),
  rate = c(.1, 1),
  shape = c(1, 2)
) %>% 
  mutate(prob = dgamma(alpha, rate = rate, shape = shape)) %>% 
  ggplot(aes(x = alpha, y = prob, group = interaction(rate, shape),
             color = factor(rate), linetype = factor(shape))) +
  geom_line() + 
  ggthemes::scale_color_colorblind() +
  labs(color = "Rate", linetype = "Shape") + 
  xlab(expression(alpha)) + 
  ylab("Prior density")
```

## Prior for modal ranking

-   Uniform distribution on $\mathcal{P}_{m}$ most natural.
-   Example, all having prior mass 1/6:

```{webr-r}
permutations(3) # from e1071 package
```

## Posterior distributions {.smaller}

::: incremental
-   Prior: $\pi(\alpha, \rho) \propto \alpha^{\gamma-1} \exp\{-\lambda \alpha\}$ with support $\mathbb{R}_{+} \times \mathcal{P}_{m}$

-   Likelihood $$
    \prod_{i=1}^{N} p(r_{i} | \alpha, \rho) = \frac{\exp\left\{-\left(\alpha/m\right) \sum_{i=1}^{N} d\left(r_{i}, \rho\right)\right\}}{Z\left(\alpha\right)^{N}}
    $$

-   Posterior $$
    P(\alpha, \rho | \text{data}) \propto \frac{\alpha^{\gamma-1}\exp\left\{-\left(\alpha/m\right) \left[\lambda +\sum_{i=1}^{N} d\left(r_{i}, \rho\right) \right]\right\}}{Z\left(\alpha\right)^{N}} 
    $$
    with support $\mathbb{R}_{+} \times \mathcal{P}_{m}$.
:::

# Computation part I

## Metropolis-Hastings {.smaller}

Goal: obtain samples from $p(\theta | r_{1:N})$.

::: incremental
1.  Initialize by picking an initial state $\theta_{0}$.
2.  Iterate for $t=0,1,2,\dots$
    a.  *Generate* a random candidate $\theta'$ from proposal distribution $g(\theta' | \theta_{t})$.
    b.  Calculate acceptance probability $$
         A\left(\theta', \theta_{t}\right) = \text{min}\left\{1, \frac{p(\theta')}{p(\theta_{t})}\frac{g(\theta_{t}|\theta')}{g(\theta' | \theta_{t})}\right\} 
         $$
    c.  Set $\theta_{t+1}=\theta'$ with probability $A(\theta',\theta_{t})$. Otherwise set $\theta_{t+1} = \theta_{t}$.
:::

. . .

Under certain assumptions the collection $\{\theta_{0},\theta_{1},\dots\}$ will approximate a sample from $p(\theta)$.

## Metropolis-Hastings for Mallows {.smaller}

::: incremental
-   Parameters $\alpha$ and $\rho$ can be dealt with one at a time.
-   Acceptance probability $A(\rho',\rho_{t})$ for proposal $\rho'$: $$
    \text{min}\left\{1, \exp\left\{-\left(\alpha_{t}/m\right) \sum_{i=1}^{N}\left[d\left(r_{i},\rho'\right) - d\left(r_{i},\rho_{t}\right)\right]\right\} \frac{g\left(\rho_{t}|\rho'\right)}{g\left(\rho'|\rho_{t}\right)}\right\}
    $$
-   Acceptance probability $A(\alpha',\alpha_{t})$ for proposal $\alpha'$: $$
    \text{min}\left\{1, \frac{\left(\alpha'\right)^{\gamma-1} Z\left(\alpha_{t}\right)^{N}}{\alpha_{t}^{\gamma-1} Z\left(\alpha'\right)^{N}} \exp\left\{-\frac{\alpha'-\alpha_{t}}{m}\left[\lambda+\sum_{i=1}^{N}d\left(r_{i},\rho_{t}\right)\right]\right\}  \frac{g\left(\alpha_{t} | \alpha'\right)}{g\left(\alpha'|\alpha_{t}\right)}\right\} 
    $$
:::

## Proposal for $\alpha$ {.smaller}

::: incremental
-   $g(\alpha'|\alpha_{t})$ should have support on $\mathbb{R}_{+}$ for efficiency.

-   @vitelliProbabilisticPreferenceLearning2017 and `BayesMallows` use lognormal distribution $$
    g\left(\alpha' | \alpha_{t}\right) \propto \frac{\exp\left\{-\left(\ln \alpha' - \ln \alpha_{t}\right) / (2 \sigma^{2})\right\}}{\alpha' \sigma}
    $$

-   Ratio becomes $$
    \frac{g\left(\alpha_{t} | \alpha'\right)}{g\left(\alpha' | \alpha_{t}\right)} = \frac{\alpha'}{\alpha_{t}}
    $$

-   Acceptance probability $$
    \text{min}\left\{1, \frac{\left(\alpha'\right)^{\gamma} Z\left(\alpha_{t}\right)^{N}}{\alpha_{t}^{\gamma} Z\left(\alpha'\right)^{N}} \exp\left\{-\frac{\alpha'-\alpha_{t}}{m}\left[\lambda+\sum_{i=1}^{N}d\left(r_{i},\rho_{t}\right)\right]\right\} \right\} 
    $$
:::

## Proposal for $\rho$

::: incremental
-   Key issue: $\rho' \in \mathcal{P}_{m}$.

-   Leap-and-shift [@vitelliProbabilisticPreferenceLearning2017] proposal; swap proposal [@crispinoBayesianMallowsApproach2019].

-   Symmetry $g(\rho' | \rho_{t}) = g(\rho_{t} | \rho')$, always for swap, for leap-and-shift with leap size 1.

-   Acceptance probability under symmetry $$
    \text{min}\left\{1, \exp\left\{-\frac{\alpha_{t}}{m} \sum_{i=1}^{N}\left[d\left(r_{i},\rho'\right) - d\left(r_{i},\rho_{t}\right)\right]\right\}\right\}
    $$
:::

## Leap-and-shift proposal

![](figures/leap_and_shift.png)


# Potato data

---

![](figures/potato.png){fig-align="center"}

## Potato data

```{r}
potato_visual
```


## Convergence diagnostics

```{r, cache=TRUE}
cl <- makeCluster(4)
mod <- compute_mallows(
  data = setup_rank_data(rankings = potato_visual), 
  compute_options = set_compute_options(nmc = 300), 
  cl = cl)
stopCluster(cl)
```

## Convergence diagnostics

```{r, cache=TRUE}
assess_convergence(mod) + scale_color_discrete(guide = "none")
```



## Convergence diagnostics

```{r, `code-line-numbers`="4", cache=TRUE}
cl <- makeCluster(4)
mod <- compute_mallows(
  data = setup_rank_data(rankings = potato_visual), 
  compute_options = set_compute_options(nmc = 5000), 
  cl = cl)
stopCluster(cl)
```

## Convergence diagnostics

```{r, cache=TRUE}
assess_convergence(mod) + scale_color_discrete(guide = "none")
```

## Convergence diagnostics

```{r, message=TRUE, cache=TRUE}
assess_convergence(mod, parameter = "rho")
```

# Studying the posteriors

## Posteriors

```{r, cache=TRUE}
burnin(mod) <- 2500
plot(mod)
```

## Posteriors

```{r, cache=TRUE}
plot(mod, parameter = "rho", items = 1:4)
```

## CP consensus

Cumulative probability consensus:

- First select the item which has the maximum a posteriori marginal probability of being ranked 1st
- Then select the item which has the maximum a posteriori marginal posterior probability of being ranked 1st or 2nd among the remaining ones.
- etc.

## CP consensus

```{r, cache=TRUE}
compute_consensus(mod)
```

# Partial rankings and preferences

## Top-3 rankings

-   Top-3 rankings: $$
    r_{i} = (\cdot , 1, \cdot, \cdot, 3, 2)
    $$

-   Possible complete rankings: $$
    (4, 1, 5, 6, 3, 2), (4, 1, 6, 5, 3, 2), \\
    (5, 1, 4, 6, 3, 2), (5, 1, 6, 4, 3, 2), \\
    (6, 1, 4, 5, 3, 2), (6, 1, 5, 4, 3, 2)
    $$

## Pairwise preferences

-   Observation $(3 \prec 2), (2 \prec 1)$

. . .

```{webr-r}
prefs <- data.frame(
  assessor = 1, bottom_item = c(2, 3), top_item = c(1, 2))
get_transitive_closure(
  setup_rank_data(preferences = prefs, n_items = 4))
```

. . .

-   Possible complete rankings (with 4 items) $$
    (4, 1, 2, 3), (1, 4, 2, 3), (1, 2, 4, 3), (1, 2, 3, 4)
    $$

## Posteriors {.smaller}

:::{.incremental}
-   Partial ranking or pairwise preferences in $y_{i}$ generate a constraint set $\mathcal{S}_{i}$.

-   Marginal likelihood $$
    \prod_{i=1}^{N} p\left(y_{i} | \alpha, \rho\right) = \frac{1}{Z\left(\alpha\right)} \prod_{i=1}^{N} \sum_{r_{i} \in \mathcal{S}_{i}} \exp\left\{-\left(\alpha /m\right)  d\left(r_{i}, \rho\right)\right\}
    $$

-   Marginal posterior $$
    p\left(\alpha, \rho | \text{data}\right) =  \frac{\alpha^{\gamma -1 } \exp\left\{-\lambda \alpha\right\}}{Z\left(\alpha\right)} \prod_{i=1}^{N} \sum_{r_{i} \in \mathcal{S}_{i}} \exp\left\{-\left(\alpha /m\right) d\left(r_{i}, \rho\right)\right\}
    $$

-   Complete data: special case with $\mathcal{S}_{i} = \{y_{i}\}$.
:::

## Metropolis-Hastings

:::{.incremental}
-   Need to add sampling of $r_{i}$.
-   Cost of $M$ iteration now becomes $\mathcal{O}(NM)$, up from $\mathcal{O}(M)$ with complete data.
-   Proposing $r_{i}$ on $\mathcal{S}_{i}$ (symmetrically), accepting with probability $$
    \exp\left\{-\frac{\alpha_{t}}{m} \sum_{i=1}^{N}\left[d\left(r_{i}', \rho_{t}\right) - d\left(r_{i,t},\rho_{t}\right)\right]\right\}
    $$
:::

## Beach data

![](figures/beach_data.png){fig-align="center"}

## Beach data

```{webr-r}
head(beach_preferences)
```

## Preparing for analysis

- Compare `dim`:

```{webr-r}
beach_data <- setup_rank_data(preferences = beach_preferences)
get_transitive_closure(beach_data)
```

## Diagnostics

```{webr-r}
mod <- compute_mallows(
  data = beach_data,
  compute_options = set_compute_options(nmc = 3, save_aug = TRUE)
)
assess_convergence(mod)
assess_convergence(mod, parameter = "rho")
assess_convergence(mod, parameter = "Rtilde")
```


## Posteriors

```{webr-r}
mod <- compute_mallows(
  data = setup_rank_data(preferences = beach_preferences),
  compute_options = set_compute_options(nmc = 3, save_aug = TRUE)
)
burnin(mod) <- 2
compute_consensus(mod)
plot_top_k(mod, k = 3)
```


# Mixtures

## Mixture modeling

- Heterogeneous population:

$$\rho_{c}, \alpha_{c}, ~c=1,\dots,C$$

- Cluster probabilities

$$\tau_{c}, ~ c=1,\dots,C$$

## Mixtures of Mallows models {.smaller}

Complete ranking, C-component mixture:

$$
p\left(r | \alpha_{1:C}, \rho_{1:C}, \tau_{1:C}\right) = \sum_{c=1}^{C}\frac{\tau_{c} \exp\left\{ - \left(\alpha_{c}/m\right) d\left(r, \rho_{c}\right)\right\}}{Z\left(\alpha_{c} \right)} 
$$

with $\sum_{c=1}^{C}\tau_{c}=1$.

. . .

-   Symmetric Dirichlet prior: $\pi(\tau_{1},\dots,\tau_{C}) \propto \prod_{c=1}^{C} \tau_{c}^{\psi-1}$.
-   Cluster labels $z_{i} \in \{1,\dots,C\}$ multinomial with probabilities $\tau_{c}$.

## Markov chain Monte Carlo {.smaller}

- Metropolis-within-Gibbs algorithm:

:::{.incremental}
1. Sample $\tau_{1}, \dots, \tau_{C} \sim \text{Dirichlet}(\psi+n_{1},\dots,\psi+n_{C})$.
2. Propose and accept/reject $\alpha_{c}$, $\rho_{c}$ for $c=1,\dots,C$ with Metropolis-Hastings targeting
$$
P\left(\alpha_{1:C}, \rho_{1:C} | z_{1:N}, \text{data}\right) \propto \\
\left[\prod_{c=1}^{C} \alpha_{c}^{\gamma_{c}-1} \exp\left\{-\lambda \alpha_{c}\right\}  \right] \left[\prod_{i=1}^{N} \frac{\exp\left\{-\left(\alpha_{z_{i}} /m\right) d\left(r_{i},\rho_{z_{i}}\right)\right\}}{Z\left(\alpha_{z_{i}}\right)} \right].
$$
3. Sample cluster labels $z_{i}$ multinomially with probabilities
$$
p_{ic} = \frac{\tau_{c} \exp\left\{-\left(\alpha_{c}/m\right) d\left(r_{i}, \rho_{c}\right)\right\}}{Z\left(\alpha_{c}\right)}.
$$
:::

## Sushi data

```{webr-r}
head(sushi_rankings)
```


## Mixtures in BayesMallows

```{r, eval=FALSE, cache=TRUE}
cl <- makeCluster(7)
mod <- compute_mallows_mixtures(
  n_clusters = 1:7,
  data = setup_rank_data(rankings = sushi_rankings),
  compute_options = set_compute_options(include_wcd = TRUE),
  cl = cl
)
stopCluster(cl)
```

## Mixtures in BayesMallows

```{r, eval=FALSE, cache=TRUE}
assess_convergence(mod)
```

![](figures/cluster_convergence.png)

## How many clusters?

```{r, eval=FALSE, cache=TRUE}
burnin(mod) <- 300
plot_elbow(mod)
```

![](figures/cluster_elbow.png)

## Five-cluster model

```{r, cache=TRUE}
mod <- compute_mallows(
  data = setup_rank_data(rankings = sushi_rankings),
  model_options = set_model_options(n_clusters = 5)
)
```

## Five-cluster model

```{r, cache=TRUE}
assess_convergence(mod)
```

```{r, cache=TRUE}
burnin(mod) <- 500
```


## Posteriors for cluster weights

```{r, cache=TRUE}
plot(mod, parameter = "cluster_probs")
```

## Where do I belong?

```{r, cache=TRUE}
plot(mod, parameter = "cluster_assignment")
```

## Cluster consensus {.smaller}

```{r, cache=TRUE}
compute_consensus(mod) %>% 
  as_tibble() %>% 
  select(-cumprob) %>% 
  pivot_wider(names_from = cluster, values_from = item)
```


# Non-transitive pairwise preferences

## Inconsistencies

@crispinoBayesianMallowsApproach2019 considers the case where pairwise preferences are inconsistent, e.g.,

$$
1 \prec 2, 2 \prec 3, 3 \prec 1
$$


```{webr-r}
prefs <- data.frame(
  assessor = 1, bottom_item = c(1, 2, 3), top_item = c(2, 3, 1)
)
prepared <- setup_rank_data(preferences = prefs)
get_transitive_closure(prepared)
```

## Inconsistent rankings

@crispinoBayesianMallowsApproach2019:

:::{.incremental}

- Assume a truth $r_{i}$ exists for each assessor,
$$
p(r_{i}) = \frac{\exp\left\{-\alpha d\left(r_{i}, \rho\right)\right\}}{Z\left(\alpha\right)}.
$$
- Inconsistencies are due to errors made by the assessor,
$$
p(A_{r} \prec A_{s} \in y_{i} | A_{s} \prec A_{r} \in r_{i}) \propto \theta.
$$

:::

## Sounds data

@barrettImpact3DSound2018:

```{r}
head(sounds, 20)
```


## Inconsistent rankings in BayesMallows

```{webr-r}
dat <- setup_rank_data(preferences = sounds)
get_transitive_closure(dat)
```

## Inconsistent rankings in BayesMallows

```{r, cache=TRUE}
mod <- compute_mallows(
  data = setup_rank_data(preferences = sounds),
  compute_options = set_compute_options(nmc = 5000),
  model_options = set_model_options(error_model = "bernoulli")
)
```

## Convergence

```{r, fig.height=5, cache=TRUE}
assess_convergence(mod)
```

## Posteriors

```{r, cache=TRUE}
burnin(mod) <- 2000
plot(mod, parameter = "theta")
```


# The end



## References
